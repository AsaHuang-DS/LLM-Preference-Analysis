{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe1befc",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "\n",
    "**Objective:** Clean both datasets, handle missing values, standardize model names, and prepare data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb456084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d30579",
   "metadata": {},
   "source": [
    "## Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5defa5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arena: (57477, 9), Kaggle: (200, 15)\n"
     ]
    }
   ],
   "source": [
    "# Load both datasets\n",
    "df_arena = pd.read_csv('../data/raw/chatbot_arena.csv')\n",
    "df_kaggle = pd.read_csv('../data/raw/llm_comparison_dataset.csv')\n",
    "\n",
    "print(f\"Arena: {df_arena.shape}, Kaggle: {df_kaggle.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a83be81",
   "metadata": {},
   "source": [
    "## Inspect Columns Names\n",
    "\n",
    "See what columns we're working with to identify model names, preferences, and prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d85ccf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arena columns:\n",
      "['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie']\n"
     ]
    }
   ],
   "source": [
    "print(\"Arena columns:\")\n",
    "print(df_arena.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24c9047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle columns:\n",
      "['Model', 'Provider', 'Context Window', 'Speed (tokens/sec)', 'Latency (sec)', 'Benchmark (MMLU)', 'Benchmark (Chatbot Arena)', 'Open-Source', 'Price / Million Tokens', 'Training Dataset Size', 'Compute Power', 'Energy Efficiency', 'Quality Rating', 'Speed Rating', 'Price Rating']\n"
     ]
    }
   ],
   "source": [
    "print(\"Kaggle columns:\")\n",
    "print(df_kaggle.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10900424",
   "metadata": {},
   "source": [
    "## Understand the Winner Columns\n",
    "\n",
    "The Arena dataset has three winner columns (`winner_model_a`, `winner_model_b`, `winner_tie`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb418e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winner_model_A unique values: [1 0]\n",
      "winner_model_B unique values: [0 1]\n",
      "winner_tie unique values: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in winner columns\n",
    "print(\"winner_model_A unique values:\", df_arena['winner_model_a'].unique())\n",
    "print(\"winner_model_B unique values:\", df_arena['winner_model_b'].unique())\n",
    "print(\"winner_tie unique values:\", df_arena['winner_tie'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eed7971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Rows where model_a won: 20064\n",
      " Rows where model_b won: 19652\n",
      " Rows where tie occurred: 17761\n"
     ]
    }
   ],
   "source": [
    "# Check how many rows have each winner type\n",
    "print(\" Rows where model_a won:\", df_arena['winner_model_a'].sum())\n",
    "print(\" Rows where model_b won:\", df_arena['winner_model_b'].sum())\n",
    "print(\" Rows where tie occurred:\", df_arena['winner_tie'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e14c86",
   "metadata": {},
   "source": [
    "## Create Single Winner Column\n",
    "\n",
    "Convert the three winner columns into one categorical column for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd29a881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winner\n",
      "model_a    20064\n",
      "model_b    19652\n",
      "tie        17761\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a single 'winner' column\n",
    "def determine_winner(row):\n",
    "    if row['winner_model_a'] == 1:\n",
    "        return 'model_a'\n",
    "    elif row['winner_model_b'] == 1:\n",
    "        return 'model_b'\n",
    "    elif row['winner_tie'] == 1:\n",
    "        return 'tie'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "    \n",
    "df_arena['winner'] = df_arena.apply(determine_winner, axis=1)\n",
    "\n",
    "# Check distribution\n",
    "print(df_arena['winner'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6936fa5e",
   "metadata": {},
   "source": [
    "## Check Model Names\n",
    "Let's see what models we have in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3cfc555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique models in Arena: 64\n",
      "\n",
      " Sample Arena models (first 10):\n",
      "1. gpt-4-0314\n",
      "2. claude-2.0\n",
      "3. openchat-3.5-0106\n",
      "4. wizardlm-13b\n",
      "5. tulu-2-dpo-70b\n",
      "6. dolphin-2.2.1-mistral-7b\n",
      "7. zephyr-7b-alpha\n",
      "8. RWKV-4-Raven-14B\n",
      "9. yi-34b-chat\n",
      "10. llama2-70b-steerlm-chat\n"
     ]
    }
   ],
   "source": [
    "# Unique models in Arena dataset\n",
    "arena_models = set(df_arena['model_a'].unique()) | set(df_arena['model_b'].unique())\n",
    "print(f\"Unique models in Arena: {len(arena_models)}\")\n",
    "print(\"\\n Sample Arena models (first 10):\")\n",
    "for i, model in enumerate(list(arena_models)[:10], 1):\n",
    "    print(f\"{i}. {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe196e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique models in Kaggle: 70\n",
      "\n",
      "Sample Kaggle models (first 10):\n",
      "  1. Mistral-3\n",
      "  2. Command-3\n",
      "  3. Claude-2\n",
      "  4. Claude-9\n",
      "  5. Llama-3\n",
      "  6. Gemini-4\n",
      "  7. Gemini-3\n",
      "  8. Nova-7\n",
      "  9. Nova-8\n",
      "  10. Llama-9\n"
     ]
    }
   ],
   "source": [
    "# Unique models in Kaggle dataset\n",
    "kaggle_models = set(df_kaggle['Model'].unique())\n",
    "print(f\"Unique models in Kaggle: {len(kaggle_models)}\")\n",
    "print(\"\\nSample Kaggle models (first 10):\")\n",
    "for i, model in enumerate(list(kaggle_models)[:10], 1):\n",
    "    print(f\"  {i}. {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d3942",
   "metadata": {},
   "source": [
    "## Find Common Models\n",
    "\n",
    "Check how many models overlap between datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70f65d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models in both datasets: 0\n",
      "Models only in Arena: 64\n",
      "Models only in Kaggle: 70\n",
      "Merge success rate: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Find common models\n",
    "common_models = arena_models & kaggle_models\n",
    "arena_only = arena_models - kaggle_models\n",
    "kaggle_only = kaggle_models - arena_models\n",
    "\n",
    "print(f\"Models in both datasets: {len(common_models)}\")\n",
    "print(f\"Models only in Arena: {len(arena_only)}\")\n",
    "print(f\"Models only in Kaggle: {len(kaggle_only)}\")\n",
    "print(f\"Merge success rate: {len(common_models) / len(arena_models) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a09c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some examples of common models\n",
    "if common_models:\n",
    "    print(\"Sample common models:\")\n",
    "    for i, model in enumerate(list(common_models)[:10], 1):\n",
    "        print(f\"  {i}. {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f4d58ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Arena-only models (first 10):\n",
      "  1. gpt-4-0314\n",
      "  2. solar-10.7b-instruct-v1.0\n",
      "  3. claude-2.0\n",
      "  4. mpt-30b-chat\n",
      "  5. vicuna-33b\n",
      "  6. nous-hermes-2-mixtral-8x7b-dpo\n",
      "  7. openchat-3.5-0106\n",
      "  8. falcon-180b-chat\n",
      "  9. wizardlm-13b\n",
      "  10. llama-13b\n"
     ]
    }
   ],
   "source": [
    "# Show examples of models only in Arena (might need name standardization)\n",
    "if arena_only:\n",
    "    print(\"\\nSample Arena-only models (first 10):\")\n",
    "    for i, model in enumerate(list(arena_only)[:10], 1):\n",
    "        print(f\"  {i}. {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f516543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Kaggle-only models (first 10):\n",
      "  1. Command-3\n",
      "  2. Mistral-3\n",
      "  3. Claude-2\n",
      "  4. Claude-9\n",
      "  5. Llama-3\n",
      "  6. Gemini-4\n",
      "  7. Gemini-3\n",
      "  8. Nova-7\n",
      "  9. Nova-8\n",
      "  10. Llama-9\n"
     ]
    }
   ],
   "source": [
    "# Show examples of models only in Kaggle (might need name standardization)\n",
    "if kaggle_only:\n",
    "    print(\"\\nSample Kaggle-only models (first 10):\")\n",
    "    for i, model in enumerate(list(kaggle_only)[:10], 1):\n",
    "        print(f\"  {i}. {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4da9f6",
   "metadata": {},
   "source": [
    "## Full Model Lists\n",
    "Let's see all models to create a standardization mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3b1031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Arena models:\n",
      "======================================================================\n",
      "  1. RWKV-4-Raven-14B\n",
      "  2. alpaca-13b\n",
      "  3. chatglm-6b\n",
      "  4. chatglm2-6b\n",
      "  5. chatglm3-6b\n",
      "  6. claude-1\n",
      "  7. claude-2.0\n",
      "  8. claude-2.1\n",
      "  9. claude-instant-1\n",
      " 10. codellama-34b-instruct\n",
      " 11. deepseek-llm-67b-chat\n",
      " 12. dolly-v2-12b\n",
      " 13. dolphin-2.2.1-mistral-7b\n",
      " 14. falcon-180b-chat\n",
      " 15. fastchat-t5-3b\n",
      " 16. gemini-pro\n",
      " 17. gemini-pro-dev-api\n",
      " 18. gpt-3.5-turbo-0125\n",
      " 19. gpt-3.5-turbo-0314\n",
      " 20. gpt-3.5-turbo-0613\n",
      " 21. gpt-3.5-turbo-1106\n",
      " 22. gpt-4-0125-preview\n",
      " 23. gpt-4-0314\n",
      " 24. gpt-4-0613\n",
      " 25. gpt-4-1106-preview\n",
      " 26. gpt4all-13b-snoozy\n",
      " 27. guanaco-33b\n",
      " 28. koala-13b\n",
      " 29. llama-13b\n",
      " 30. llama-2-13b-chat\n",
      " 31. llama-2-70b-chat\n",
      " 32. llama-2-7b-chat\n",
      " 33. llama2-70b-steerlm-chat\n",
      " 34. mistral-7b-instruct\n",
      " 35. mistral-7b-instruct-v0.2\n",
      " 36. mistral-medium\n",
      " 37. mixtral-8x7b-instruct-v0.1\n",
      " 38. mpt-30b-chat\n",
      " 39. mpt-7b-chat\n",
      " 40. nous-hermes-2-mixtral-8x7b-dpo\n",
      " 41. oasst-pythia-12b\n",
      " 42. openchat-3.5\n",
      " 43. openchat-3.5-0106\n",
      " 44. openhermes-2.5-mistral-7b\n",
      " 45. palm-2\n",
      " 46. pplx-70b-online\n",
      " 47. pplx-7b-online\n",
      " 48. qwen-14b-chat\n",
      " 49. qwen1.5-4b-chat\n",
      " 50. qwen1.5-72b-chat\n",
      " 51. qwen1.5-7b-chat\n",
      " 52. solar-10.7b-instruct-v1.0\n",
      " 53. stablelm-tuned-alpha-7b\n",
      " 54. starling-lm-7b-alpha\n",
      " 55. stripedhyena-nous-7b\n",
      " 56. tulu-2-dpo-70b\n",
      " 57. vicuna-13b\n",
      " 58. vicuna-33b\n",
      " 59. vicuna-7b\n",
      " 60. wizardlm-13b\n",
      " 61. wizardlm-70b\n",
      " 62. yi-34b-chat\n",
      " 63. zephyr-7b-alpha\n",
      " 64. zephyr-7b-beta\n",
      "\n",
      "Total: 64 models\n"
     ]
    }
   ],
   "source": [
    "# Show all Arena models\n",
    "print(\"All Arena models:\")\n",
    "print(\"=\" * 70)\n",
    "arena_models_sorted = sorted(list(arena_models))\n",
    "for i, model in enumerate(arena_models_sorted, 1):\n",
    "    print(f\"{i: 3d}. {model}\")\n",
    "print(f\"\\nTotal: {len(arena_models_sorted)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b94a299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL KAGGLE MODELS:\n",
      "======================================================================\n",
      "  1. Claude-1\n",
      "  2. Claude-2\n",
      "  3. Claude-3\n",
      "  4. Claude-5\n",
      "  5. Claude-6\n",
      "  6. Claude-7\n",
      "  7. Claude-8\n",
      "  8. Claude-9\n",
      "  9. Command-1\n",
      " 10. Command-2\n",
      " 11. Command-3\n",
      " 12. Command-4\n",
      " 13. Command-5\n",
      " 14. Command-6\n",
      " 15. Command-7\n",
      " 16. Command-8\n",
      " 17. Command-9\n",
      " 18. DeepSeek-1\n",
      " 19. DeepSeek-2\n",
      " 20. DeepSeek-3\n",
      " 21. DeepSeek-4\n",
      " 22. DeepSeek-5\n",
      " 23. DeepSeek-6\n",
      " 24. DeepSeek-7\n",
      " 25. DeepSeek-8\n",
      " 26. DeepSeek-9\n",
      " 27. GPT-1\n",
      " 28. GPT-2\n",
      " 29. GPT-3\n",
      " 30. GPT-4\n",
      " 31. GPT-5\n",
      " 32. GPT-6\n",
      " 33. GPT-7\n",
      " 34. GPT-8\n",
      " 35. GPT-9\n",
      " 36. Gemini-1\n",
      " 37. Gemini-2\n",
      " 38. Gemini-3\n",
      " 39. Gemini-4\n",
      " 40. Gemini-5\n",
      " 41. Gemini-6\n",
      " 42. Gemini-7\n",
      " 43. Gemini-8\n",
      " 44. Gemini-9\n",
      " 45. Llama-1\n",
      " 46. Llama-2\n",
      " 47. Llama-3\n",
      " 48. Llama-4\n",
      " 49. Llama-5\n",
      " 50. Llama-6\n",
      " 51. Llama-7\n",
      " 52. Llama-8\n",
      " 53. Llama-9\n",
      " 54. Mistral-1\n",
      " 55. Mistral-2\n",
      " 56. Mistral-3\n",
      " 57. Mistral-4\n",
      " 58. Mistral-5\n",
      " 59. Mistral-6\n",
      " 60. Mistral-7\n",
      " 61. Mistral-8\n",
      " 62. Nova-1\n",
      " 63. Nova-2\n",
      " 64. Nova-3\n",
      " 65. Nova-4\n",
      " 66. Nova-5\n",
      " 67. Nova-6\n",
      " 68. Nova-7\n",
      " 69. Nova-8\n",
      " 70. Nova-9\n",
      "\n",
      "Total: 70 models\n"
     ]
    }
   ],
   "source": [
    "# Show all Kaggle models\n",
    "print(\"ALL KAGGLE MODELS:\")\n",
    "print(\"=\"*70)\n",
    "kaggle_models_sorted = sorted(list(kaggle_models))\n",
    "for i, model in enumerate(kaggle_models_sorted, 1):\n",
    "    print(f\"{i:3d}. {model}\")\n",
    "print(f\"\\nTotal: {len(kaggle_models_sorted)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59401be9",
   "metadata": {},
   "source": [
    "## Analysis Update\n",
    "\n",
    "**Issue:** The Kaggle dataset contains sythetic model names that don't match the real Arena models. <br>\n",
    "**New Approach:** Focus the analysis on the Arena dataset (which has real user data) and skip the merge with Kaggle. There is still 57,000+ converstions with 64 real models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a05bde",
   "metadata": {},
   "source": [
    "## Handle Missing Values - Arena Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db1050f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in Arena dataset:\n",
      "Empty DataFrame\n",
      "Columns: [Missing Count, Missing %]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check missing values\n",
    "missing = df_arena.isnull().sum()\n",
    "missing_pct = (missing / len(df_arena) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing %': missing_pct\n",
    "})\n",
    "\n",
    "print(\"Missing values in Arena dataset:\")\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee3ee6",
   "metadata": {},
   "source": [
    "## Remove Rows with Missing Critical Data\n",
    "Drop rows where prompt or model names are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb4008d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_a: 0 missing (0.00%)\n",
      "model_b: 0 missing (0.00%)\n",
      "prompt: 0 missing (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Check how many rows have missing critical data\n",
    "critical_cols = ['model_a', 'model_b', 'prompt']\n",
    "rows_before = len(df_arena)\n",
    "\n",
    "for col in critical_cols:\n",
    "    missing_count = df_arena[col].isnull().sum()\n",
    "    print(f\"{col}: {missing_count} missing ({missing_count/rows_before*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e55cef9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows before: 57,477\n",
      "Rows after: 57,477\n",
      "Rows dropped: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing critical columns\n",
    "df_arena_clean = df_arena.dropna(subset=critical_cols)\n",
    "\n",
    "rows_after = len(df_arena_clean)\n",
    "rows_dropped = rows_before - rows_after\n",
    "\n",
    "print(f\"\\nRows before: {rows_before:,}\")\n",
    "print(f\"Rows after: {rows_after:,}\")\n",
    "print(f\"Rows dropped: {rows_dropped:,} ({rows_dropped/rows_before*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5d171",
   "metadata": {},
   "source": [
    "## Remove Duplicate Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1c4472e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate conversations: 126 (0.22%)\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates based on prompt and models\n",
    "duplicates = df_arena_clean.duplicated(subset=['prompt', 'model_a', 'model_b'])\n",
    "num_duplicates = duplicates.sum()\n",
    "\n",
    "print(f\"Duplicate conversations: {num_duplicates:,} ({num_duplicates/len(df_arena_clean)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46c329f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after removing duplicates: 57,351\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "df_arena_clean = df_arena_clean.drop_duplicates(subset=['prompt', 'model_a', 'model_b'])\n",
    "\n",
    "print(f\"Rows after removing duplicates: {len(df_arena_clean):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0235d8",
   "metadata": {},
   "source": [
    "## Create Prompt Length Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ee8eee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt length statistics:\n",
      "count    57351.000000\n",
      "mean       368.842130\n",
      "std       1073.962642\n",
      "min          7.000000\n",
      "25%         52.000000\n",
      "50%         96.000000\n",
      "75%        242.000000\n",
      "max      33056.000000\n",
      "Name: prompt_length, dtype: float64\n",
      "\n",
      "Word count statistics:\n",
      "count    57351.000000\n",
      "mean        53.940594\n",
      "std        143.983150\n",
      "min          1.000000\n",
      "25%          9.000000\n",
      "50%         16.000000\n",
      "75%         39.000000\n",
      "max       4719.000000\n",
      "Name: prompt_word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate prompt length (characters)\n",
    "df_arena_clean['prompt_length'] = df_arena_clean['prompt'].str.len()\n",
    "\n",
    "# Calculate word count\n",
    "df_arena_clean['prompt_word_count'] = df_arena_clean['prompt'].str.split().str.len()\n",
    "\n",
    "print(\"Prompt length statistics:\")\n",
    "print(df_arena_clean['prompt_length'].describe())\n",
    "print(\"\\nWord count statistics:\")\n",
    "print(df_arena_clean['prompt_word_count'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e8e8fb",
   "metadata": {},
   "source": [
    "## Create Response Length Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a7cf1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response A length:\n",
      "count    57351.000000\n",
      "mean      1379.571446\n",
      "std       1514.849236\n",
      "min          4.000000\n",
      "25%        409.000000\n",
      "50%       1079.000000\n",
      "75%       1864.000000\n",
      "max      54058.000000\n",
      "Name: response_a_length, dtype: float64\n",
      "\n",
      "Response B length:\n",
      "count    57351.000000\n",
      "mean      1387.991386\n",
      "std       1538.790085\n",
      "min          4.000000\n",
      "25%        414.000000\n",
      "50%       1088.000000\n",
      "75%       1874.000000\n",
      "max      53830.000000\n",
      "Name: response_b_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Response lengths\n",
    "df_arena_clean['response_a_length'] = df_arena_clean['response_a'].str.len()\n",
    "df_arena_clean['response_b_length'] = df_arena_clean['response_b'].str.len()\n",
    "\n",
    "print(\"Response A length:\")\n",
    "print(df_arena_clean['response_a_length'].describe())\n",
    "print(\"\\nResponse B length:\")\n",
    "print(df_arena_clean['response_b_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b1b9bd",
   "metadata": {},
   "source": [
    "## Save Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d45dc26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to: ../data/processed/arena_cleaned.csv\n",
      "Shape: (57351, 14)\n",
      "Columns: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'winner', 'prompt_length', 'prompt_word_count', 'response_a_length', 'response_b_length']\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned dataset\n",
    "output_path = '../data/processed/arena_cleaned.csv'\n",
    "df_arena_clean.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to: {output_path}\")\n",
    "print(f\"Shape: {df_arena_clean.shape}\")\n",
    "print(f\"Columns: {df_arena_clean.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "761dfc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Summary:\n",
      "======================================================================\n",
      "Original rows: 57,477\n",
      "After removing missing values: 57,477\n",
      "After removing duplicates: 57,351\n",
      "Final dataset: 57,351 rows * 14 columns\n",
      "Data retention: 99.8%\n"
     ]
    }
   ],
   "source": [
    "# Summary of cleaning\n",
    "print(\"Cleaning Summary:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Original rows: {rows_before:,}\")\n",
    "print(f\"After removing missing values: {rows_after:,}\")\n",
    "print(f\"After removing duplicates: {len(df_arena_clean):,}\")\n",
    "print(f\"Final dataset: {len(df_arena_clean):,} rows * {df_arena_clean.shape[1]} columns\")\n",
    "print(f\"Data retention: {len(df_arena_clean)/rows_before*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5521fd71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
